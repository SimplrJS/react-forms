{"version":3,"sources":["test/Tokenizer.test.ts"],"names":[],"mappings":";AAAA,4FAA4F;AAC5F,2DAA2D;;;;;;;;;;;;AAE3D,+BAA+B;AAE/B,6BAA8B;AAC9B,wCAAmC;AACnC,wDAAmD;AACnD,kCAA4C;AAC5C,0CAAqC;AAErC,uFAAuF;AAEvF;;GAEG;AACH;IAA4B,iCAAS;IACnC,uBAAY,IAAY,EAAE,WAAsC;eAC9D,kBAAM,IAAI,EAAE,WAAW,CAAC;IAC1B,CAAC;IAEM,oCAAY,GAAnB,UAAoB,IAAY;QAC9B,MAAM,CAAC,IAAI,CAAC,aAAa,CAAC,IAAI,CAAC,CAAC;IAClC,CAAC;IAEM,sCAAc,GAArB,UAAsB,IAAY;QAChC,MAAM,CAAC,IAAI,CAAC,eAAe,CAAC,IAAI,CAAC,CAAC;IACpC,CAAC;IACH,oBAAC;AAAD,CAZA,AAYC,CAZ2B,mBAAS,GAYpC;AAED,QAAQ,CAAC,iBAAiB,EAAE;IAC1B,IAAI,CAAC,OAAO,CAAC,KAAK,CAAC,CAAC;IAEpB,QAAQ,CAAC,mBAAmB,EAAE;QAC5B,IAAM,aAAa,GAAkB,IAAI,aAAa,CAAC,EAAE,EAAE,OAAO,CAAC,GAAG,CAAC,CAAC;QAExE,EAAE,CAAC,gBAAgB,EAAE;YACnB,IAAM,IAAI,GAAW,gLAC6D,CAAC;YAEnF,IAAM,cAAc,GAAY;gBAC9B,IAAI,eAAK,CAAC,iBAAS,CAAC,IAAI,EAAE,EAAE,EAAE,8BAA8B,CAAC;gBAC7D,IAAI,eAAK,CAAC,iBAAS,CAAC,QAAQ,EAAE,OAAO,CAAC;gBACtC,IAAI,eAAK,CAAC,iBAAS,CAAC,IAAI,EAAE,EAAE,EAAE,IAAI,CAAC;gBACnC,IAAI,eAAK,CAAC,iBAAS,CAAC,QAAQ,EAAE,OAAO,CAAC;gBACtC,IAAI,eAAK,CAAC,iBAAS,CAAC,IAAI,EAAE,EAAE,EAAE,qCAAqC,CAAC;gBACpE,IAAI,eAAK,CAAC,iBAAS,CAAC,QAAQ,EAAE,OAAO,CAAC;gBACtC,IAAI,eAAK,CAAC,iBAAS,CAAC,IAAI,EAAE,EAAE,EAAE,SAAS,CAAC;gBACxC,IAAI,eAAK,CAAC,iBAAS,CAAC,IAAI,EAAE,EAAE,EAAE,iCAAiC,CAAC;aACjE,CAAC;YAEF,IAAM,YAAY,GAAY,aAAa,CAAC,YAAY,CAAC,IAAI,CAAC,CAAC;YAC/D,kBAAQ,CAAC,YAAY,CAAC,iCAAiC,EAAE,IAAI,CAAC,SAAS,CAAC,cAAc,CAAC,CAAC,CAAC;YACzF,kBAAQ,CAAC,YAAY,CAAC,+BAA+B,EAAE,IAAI,CAAC,SAAS,CAAC,YAAY,CAAC,CAAC,CAAC;YACrF,0BAAgB,CAAC,yBAAyB,CAAC,+BAA+B,EAAE,iCAAiC,CAAC,CAAC;QACjH,CAAC,CAAC,CAAC;QAEH,EAAE,CAAC,kBAAkB,EAAE;YACrB,IAAM,KAAK,GAAW,2CAA2C,CAAC;YAClE,IAAM,aAAa,GAAU,IAAI,eAAK,CAAC,iBAAS,CAAC,SAAS,EAAE,OAAO,EAAE,yBAAyB,CAAC,CAAC;YAChG,IAAM,WAAW,GAAU,aAAa,CAAC,cAAc,CAAC,KAAK,CAAC,CAAC;YAC/D,aAAM,CAAC,KAAK,CAAC,aAAa,CAAC,IAAI,EAAE,WAAW,CAAC,IAAI,CAAC,CAAC;YACnD,aAAM,CAAC,KAAK,CAAC,aAAa,CAAC,GAAG,EAAE,WAAW,CAAC,GAAG,CAAC,CAAC;YACjD,aAAM,CAAC,KAAK,CAAC,aAAa,CAAC,IAAI,EAAE,WAAW,CAAC,IAAI,CAAC,CAAC;QACrD,CAAC,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC","file":"test/Tokenizer.test.js","sourcesContent":["// Copyright (c) Microsoft Corporation. All rights reserved. Licensed under the MIT license.\r\n// See LICENSE in the project root for license information.\r\n\r\n/// <reference types=\"mocha\" />\r\n\r\nimport { assert } from 'chai';\r\nimport JsonFile from '../JsonFile';\r\nimport TestFileComparer from '../TestFileComparer';\r\nimport Token, { TokenType } from '../Token';\r\nimport Tokenizer from '../Tokenizer';\r\n\r\n/* tslint:disable:no-function-expression - Mocha uses a poorly scoped \"this\" pointer */\r\n\r\n/**\r\n * Dummy class wrapping Tokenizer to test its protected methods\r\n */\r\nclass TestTokenizer extends Tokenizer {\r\n  constructor(docs: string, reportError: (message: string) => void) {\r\n    super(docs, reportError);\r\n  }\r\n\r\n  public tokenizeDocs(docs: string): Token[] {\r\n    return this._tokenizeDocs(docs);\r\n  }\r\n\r\n  public tokenizeInline(docs: string): Token {\r\n    return this._tokenizeInline(docs);\r\n  }\r\n}\r\n\r\ndescribe('Tokenizer tests', function (): void {\r\n  this.timeout(10000);\r\n\r\n  describe('Tokenizer methods', function (): void {\r\n    const testTokenizer: TestTokenizer = new TestTokenizer('', console.log);\r\n\r\n    it('tokenizeDocs()', function (): void {\r\n      const docs: string = `this is a mock documentation\\n @taga hi\\r\\n @tagb hello @invalid@tag email@domain.com\r\n        @tagc this is {   @inlineTag param1  param2   } and this is {just curly braces}`;\r\n\r\n      const expectedTokens: Token[] = [\r\n        new Token(TokenType.Text, '', 'this is a mock documentation'),\r\n        new Token(TokenType.BlockTag, '@taga'),\r\n        new Token(TokenType.Text, '', 'hi'),\r\n        new Token(TokenType.BlockTag, '@tagb'),\r\n        new Token(TokenType.Text, '', 'hello @invalid@tag email@domain.com'),\r\n        new Token(TokenType.BlockTag, '@tagc'),\r\n        new Token(TokenType.Text, '', 'this is'),\r\n        new Token(TokenType.Text, '', 'and this is {just curly braces}')\r\n      ];\r\n\r\n      const actualTokens: Token[] = testTokenizer.tokenizeDocs(docs);\r\n      JsonFile.saveJsonFile('./lib/tokenizeDocsExpected.json', JSON.stringify(expectedTokens));\r\n      JsonFile.saveJsonFile('./lib/tokenizeDocsActual.json', JSON.stringify(actualTokens));\r\n      TestFileComparer.assertFileMatchesExpected('./lib/tokenizeDocsActual.json', './lib/tokenizeDocsExpected.json');\r\n    });\r\n\r\n    it('tokenizeInline()', function (): void {\r\n      const token: string = '{    @link   https://bing.com  |  Bing  }';\r\n      const expectedToken: Token = new Token(TokenType.InlineTag, '@link', 'https://bing.com | Bing');\r\n      const actualToken: Token = testTokenizer.tokenizeInline(token);\r\n      assert.equal(expectedToken.type, actualToken.type);\r\n      assert.equal(expectedToken.tag, actualToken.tag);\r\n      assert.equal(expectedToken.text, actualToken.text);\r\n    });\r\n  });\r\n});\r\n"],"sourceRoot":"..\\..\\src"}